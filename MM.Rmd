---
title: "Les  forêts aléatoires méritent des explications"
author: "CB"
date: "23 décembre 2018"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
  number_sections: true
bibliography: MM.bib
---
![Miro](MLMiro.jpg)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,include=TRUE,message=FALSE,warning=FALSE)
```

La principale différence entre le machine learning et la modélisation statistique est que si dans le second cas on teste l'ajustement d'un modèle qui dérive d'hypothèses théorique à un jeu de donnée approprié, dans le premier cas on teste la prédiction sur un échantillon test qui ne participe pas à l'entrainement du modèle. 

Ce qui est gagné par le machine learning prédictif est perdu dans le caractère boite noire des modèles qu'il produit, et c'est aujourd'hui un enjeu pour les chercheurs que de faire que les machines non seulement prédisent mais expliquent aussi leur prédiction. C'est désormais un enjeu et on lira avec intérêt la synthèse de @liu_towards_2017.

Le processus général de machine learning prend la forme suivante, on en suivra les grandes lignes (sauf la généralisation) : entrainer un modèle, le tester et l'évaluer, l'expliciter globalement et localement.
![processus du machine learning](processusML.jpg)

##Les outils

Des outils récents permettent de le faire , notamment `lime` qui sera utilisé dans notre illustration. On s'inspire très largement de [ce document de lime ](https://uc-r.github.io/lime) pour le code. `vip`et `pdp` sont des aides à l'interprétation globale, lime est destiner à comprendre des cas individuels. Autrement dit à expliciter pourquoi l'algo attribue la probabilité d'avoir le trait prédit. 

On emploie `[caret](http://topepo.github.io/caret/)` (plusieurs centaines de modèles sont disponibles!) ainsi que `[H20](http://docs.h2o.ai/h2o-tutorials/latest-stable/)` qui propose des méthodes avancées de machine learning y compris du deep learning .

On s'appuie dans cette exercice principalement sur des random forests. 

les données sont [ici](https://drive.google.com/file/d/1DBOfwhLoy2JpPZP-sN1xMwpxkgyX1A1V/view?usp=sharing) et le script [là](https://drive.google.com/file/d/1Z5atm-WbkkjilCF5lWp5a_djB_Slo_xw/view?usp=sharing) pour reproduire et jouer avec les analyses.


```{r Step01}
library(readr)
T_csv <- read_csv("~/Machine learning/T.csv.txt") #adaptez le chemin.

library(rsample)    # pour la manipulation des données
library(dplyr)      # pour la manipulation des données
library(lime)       # ML local interpretation
library(vip)        # ML global interpretation
library(pdp)        # ML global interpretation
library(ggplot2)    # visualization pkg leveraged by above packages
library(caret)      # ML model building
library(h2o)        # ML model building
library(knitr)
library(kableExtra)
h2o.init()
## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpIqxdOK/h2o_bradboehmke_started_from_r.out
##     /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpIqxdOK/h2o_bradboehmke_started_from_r.err
h2o.no_progress()
```

##Constitution des sets de données

Dans une première étape on selectionne les variables qui nous intéresse et on dichotomise la variable que l'on cherche à prédire : être ou ne pas être heureux ( si>7), et l'on filtre les données manquantes.

Pour diviser le fichier en un échantillon d'entrainement et un échantillon de test, on crée un index, puis on sélectionne les lignes en fonction de cet index. 


```{r data01}
df <- T_csv %>% 
  dplyr::mutate_if(is.ordered, factor, ordered = FALSE) %>%
  dplyr::mutate(happy, happy2 = ifelse(happy > 7, "heureux", "malheureux"))%>%
  select(-happy,-edulvlb,-stflife)%>%drop_na()
df$happy2<-as.factor(df$happy2) 

Tab <- xtabs(~happy2, data=df)
kable(margin.table(Tab, 1),digit=3)%>%
  kable_styling()

index <- 1:6000
train_obs <- df[-index, ]
test_obs <- df[index, ]

```

##Random forest avec H2o

Il suffit de définir x et y, de formater les deux jeux de données pour H2o et ensuite d'appeler les fonctions : Dans notre cas :
 - Random forests : le principe consiste à générer à parti de sous échantillons des dizaines voir des centaines d'arbres de décision pui de les moyenner. Pour une présentation complète on lira : 
 - GLM : generalized linear model
 - gbm : Gradient Boosting Model : analogues aux RF, mais dont les différents arbres sont construit sur la base sur précédents dans une stratégie de réduction des erreurs.
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html


```{r data02}
y <- "happy2"
x <- setdiff(names(train_obs), y)
train_obs.h2o <- as.h2o(train_obs)
test_obs.h2o <- as.h2o(test_obs)

# create h2o models
h2o_rf <- h2o.randomForest(x, y, training_frame = train_obs.h2o)
h2o_glm <- h2o.glm(x, y, training_frame = train_obs.h2o, family = "binomial")
h2o_gbm <- h2o.gbm(x, y, training_frame = train_obs.h2o)

```


##Random forest avec caret

ranger est une mméthode de ranfom forest qui est gérée par le package CARet

https://cran.r-project.org/web/packages/caret/vignettes/caret.html



```{r Step04}
fit.caret <- train(
  happy2 ~ ., 
  data = train_obs, 
  method = 'ranger',
  trControl = trainControl(method = "cv", number = 5, classProbs = TRUE),
  tuneLength = 1,
  importance = 'impurity'
  )


# ranger model --> model type not built in to LIME
fit.ranger <- ranger::ranger(
  happy2 ~ ., 
  data = train_obs, 
  importance = 'impurity',
  probability = TRUE
)
```

##Evaluation de la qualité de la prédiction


###Un premier coup d'oeil

Examinons d'abord comment les probabilités calculées d'être heureux se distribuent selon les deux modalités de notre critère. 

```{r predic01}
print(fit.caret)
# Predictions
pred<- predict(fit.caret, test_obs, type = "prob")
df2<-cbind(test_obs,pred)

# Show src data and predictions
library(ggplot2);
g <- ggplot(df2, aes(x = happy2, y = heureux)) + geom_violin()
g


```

###courbes roc
on emploie maintenant la library [ROCR](https://hopstat.wordpress.com/2014/12/19/a-small-introduction-to-the-rocr-package/), qui nous permet de représenter la qualité du modèle, mais aussi de détecter le seuil de décision. 

```{r predic02}
#analyse ROC 
library(ROCR)


pred1 <- prediction(df2$heureux,df2$happy2)
class(pred1)
#faux et vrais positifs
roc.perf1 = performance(pred1, measure = "fpr", x.measure = "tpr")
heatcols <- heat.colors(10)
plot(roc.perf1, colorize = TRUE,colorkey=TRUE,colorize.palette=heatcols,lwd=5)
abline(a=0, b= 1)
#recall precision
perf1 <- performance(pred1, "prec", "rec")
plot(perf1, colorize = TRUE,colorkey=TRUE,colorize.palette=heatcols,lwd=4)
```

La fonction suivante permet de  détecter le seuil de décision optimal qui est de 48,85%. On calcule la décision avec ce seuil et en croisant avec le véritable classement on constate que le taux de bon classement est de l'ordre de 63%.


```{r predic02b}
opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

x<-opt.cut(roc.perf1, pred1)
kable(x) %>%
  kable_styling()

df2$happy_pred<-"malheureux"
df2$happy_pred[df2$heureux>0.4885]<-"heureux"

Tab <- xtabs(~happy2+happy_pred, data=df2)
kable(prop.table(Tab, 1),digit=2)%>%
  kable_styling()

```
Enfin les diagrammes qui indiquent comment la précision et le recall varient en fonction du seuil sont  çi-dessous.

```{r predic02c}
par(mfrow=c(3,1))

acc.perf = performance(pred1, measure = "rec")
plot(acc.perf)

acc.perf = performance(pred1, measure = "f")
plot(acc.perf)

acc.perf = performance(pred1, measure = "acc")
plot(acc.perf)


```


##Interprêter le modèle globalement

prédire est bien, expliquer est mieux. En dépit du caractère boite noire du ML, des tentatives sont faite pour l'évaluer globalement, identifier les facteurs qui pèses le plus, et les effets relatifs des modalités d'un même facteur. on consultera @goldstein_peeking_2015 sur ce sujet et en particulier pour les ICE plots.


###L'importance des critères

Caret propose un premier outil d'analyse qui calcule l'importance relative des critères (selon différentes méthodes en fonction de la nature des variables et des modèles).

Dans notre cas la santé domine, mais les autres critères ont une importance presque équivalente, il n'y a pas une variable du bonheur mais une accumulation de facteurs.


```{r lime}
vip(fit.caret) + ggtitle("ranger: RF")
```


###Diagramme partiel (pdp)

Ils permettent d'apprécier l'impact de chacun des critères potentiellement prédictifs. On s'aperçoit ainsi que l'état de santé acccroit la probabilité de 0.30 à près de 0.60 d'être heureux selon que se sente en très mauvaise santé ou en très bonne. L'allure de la courbe montre cependant que cet effet plafonne. Pour l'age c'est une relation presque linéaire et décroissante, les jeunes ont moins de chance d'être heureux que les plus vieux, avec cependant des sorte de marches : ceux qui sont nés dans les années 30/40 (les 75 ans au moment de l'enquête) et ceux nés dans les années 80 (ils approchent de la quarantaine).


```{r step05b}

h2o.partialPlot(h2o_rf, data = train_obs.h2o, cols = "health")
h2o.partialPlot(h2o_rf, data = train_obs.h2o, cols = "yrbrn")

```
###ICE plot

les Ice Plots of Individual Conditional Expectation sont construit en calculant pour chaque individu (et donc en fonction de son profil de critère) les probabilités qu'il soit , ici heureux, pour chacun des niveaux de la variables d'intérêt. On represente celà graphiquement, et chaque ligne conrrespond au profil de variation de la probabilité d'être heureux selon, dans notre cas, l'age de l'individu, toutes autres caractéristiques égales.

Dans l'exemple suivant ces espérances conditionnelles individuelles sont de plus centré pour mieux faire apparaitre l'hétérogénéité. Effectivement on se rend compte qu'autant de courbes décroissantes (la probabilité d'être heureux diminue si on est plus jeune) que croissante sont observées. Il y a comme deux faisceaux qui se sépare dans la génération des années soixantes. 

Ceci est l'effet sans doute d'une intéraction avec d'autres variables. ( à creuser)

```{r lime02}

fit.caret %>%
  partial(pred.var = "yrbrn", grid.resolution = 25, ice = TRUE) %>%
  autoplot(rug = TRUE, train = train_obs, alpha = 0.05, center = TRUE)
```

##Expliquer localement les prédictions avec lime

Dans la dernière étape il s'agit de comprendre comment le modèle calcule les probabilités d'avoir le caractère étudié pour une prédiction donnée. Il s'agit donc localement de comprendre comment les caractéristiques (features) expliquent la prédiction réalisée. C'est l'objet de la méthode `lime` ou ,  Local Interpretable Model-agnostic Explaination, developpée par @ribeiro_why_2016 dont le graphique suivant explique le principe.

![Principe Lime](Lime.jpg)

Le principe général est relativement simple : utiliser un modèle local pour approximer de manière interprétable un modèle global. Il consiste à identifier un certain nombre de cas voisins (permutations) et sur cet ensemble (les observations sont pondérées en fonction de leur distance au point étudié), à estimer un modèle interprétable qui associe les prédicteurs à leurs prédictions (obtenue par le classificateur global qu'on cherche à évaluer). Un modèle interprétable est par exemple une régression lasso (@tibshirani_regression_2011) dont l'intérêt est d'obtenir des paramètres pour un nombre réduit de critères grace à la normalisation (la somme des paramètres est contrainte à être inférieure à une certaines valeur).

Les paramètres du modèles sont donc le nombre de permutations, un paramètre de largeur du kernell et le nombre de prédicteurs (features) que l'on souhaite.

```{r lime03}
explainer_caret <- lime(train_obs, h2o_rf, n_bins = 5)

class(explainer_caret)
x<-test_obs[2:5,]

summary(explainer_caret)

explanation_caret <- explain(x = x, 
  explainer = explainer_caret, 
  n_permutations = 500,
  dist_fun = "euclidean",
  kernel_width = .85,
  n_features = 5, 
  feature_select = "highest_weights",
  labels = "heureux"
  )
plot_features(explanation_caret)
```

Les résultats se présentent sous la forme de trois éléments pour chaque cas étudié 
 1) la probabilité qu'il soit heureux
 2) Un indicateur d' ajustement de l'explication : ainsi dans le cas 4 celle-ci est bonne et permet donc bien de comprendre pourquoi une forte probabilité a été calculée.
 3) les modalités des critères avec leur contribution relative à l'explication, soit parqu'ils soutiennent la prédiction ou qu'ils la contredisent. Dans le cas 2 : les critères se contredisent et le degré de confiance à accorder à l'explication est faible.


##Conclusion : ver un ML interactif

Dans notre exemple, il y a peu d'intérêt à analyser chaque cas. Ce pourrait être plus judicieux si nous nous intéressions à des profils types. (par exemple étudier les jeunes femmes detenant un master).

Ca l'est encore plus quand on peut confronter la prédiction de la machine au jugement de l'expert. Par exemple la prédiction d'un cancer à partir de l'analyse des caractéristiques de certaines cellules pourra mieux être appréciée par l'oncologue, si la machine indique à celui ci les raison et la consistance de sa prédiction. 

Dans l'usage du ML, on passe ainsi d'une mise en oeuvre brutale qui plaque ces prédictions, à des systèmes interactifs où le dialogue entre la connaissance statistique de la machine et celles théoriques et empirique de l'expert devient l'élément principal de la prise de décision. Ce dialogue peut aussi naturellement conduire à reformuler les modèles.

##Références

